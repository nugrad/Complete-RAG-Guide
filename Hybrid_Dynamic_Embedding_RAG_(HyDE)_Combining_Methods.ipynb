{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "Hybrid Dynamic Embedding RAG (HyDE) is an advanced variant of Retrieval-Augmented Generation (RAG) that combines hybrid retrieval methods (dense and sparse retrieval) with dynamic embedding generation to improve document retrieval and response generation.\n",
        "\n",
        "Unlike traditional RAG, which relies solely on vector-based retrieval, HyDE leverages multiple retrieval strategies and dynamic embedding transformations to optimize search results, making it particularly effective for long-tail queries, multi-turn dialogues, and low-resource domains."
      ],
      "metadata": {
        "id": "lXEnIsIKyl5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Concepts of HyDE**\n",
        "**HyDE builds upon three core principles:**\n",
        "\n",
        "\n",
        "## **1. Hybrid Retrieval: Combining Dense and Sparse Methods**\n",
        "* HyDE integrates:Dense Retrieval (Vector Search): Uses embeddings from models like FAISS, OpenAI embeddings, or BERT-based embeddings to retrieve semantically similar documents.\n",
        "* Sparse Retrieval (BM25, Keyword Search): Uses traditional term frequency (TF-IDF, BM25) methods to capture keyword-based relevance.\n",
        "\n",
        "## **2. Dynamic Embedding Generation**\n",
        "\n",
        "* Instead of relying on static embeddings, HyDE dynamically generates query-specific embeddings based on synthetic document generation:\n",
        "\n",
        "  * The LLM first hallucinates a potential response based on the query.\n",
        "  * This response is embedded and used as a query to retrieve more relevant documents.\n",
        "  * The retrieved documents refine the final response.\n",
        "\n",
        "# **3. Self-Improving Iterative Retrieval**\n",
        "* HyDE iteratively re-evaluates the retrieved documents to improve query resolution.\n",
        "* Multiple retrieval passes refine and expand the knowledge base.\n"
      ],
      "metadata": {
        "id": "X9TR0ascyw20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Applications of HyDE**\n",
        "1. Legal & Research Applications\n",
        "  * Improves retrieval of complex case laws and research articles by combining semantic understanding and keyword-based matching.\n",
        "\n",
        "2. Enterprise Search Systems\n",
        "  * Enhances internal knowledge base searches, reducing irrelevant document retrieval.\n",
        "\n",
        "3. Medical and Scientific Literature\n",
        "  * Retrieves highly relevant documents using dynamic embeddings that adapt to medical terminology.\n",
        "\n",
        "4. Open-Domain Q&A Systems\n",
        "  * Effective in multi-turn dialogues, refining responses dynamically.\n",
        "\n",
        "5. AI-Powered Code Search\n",
        "  * Enhances API documentation retrieval by matching queries with sparse keywords and semantic embeddings."
      ],
      "metadata": {
        "id": "JV-AkxI5zjgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation**"
      ],
      "metadata": {
        "id": "zxtCBlwdz3T7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2elT1ZGMuVwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143d7779-6571-49c4-8469-9497a94f17ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following yanked versions: 20081119\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement hashlib (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for hashlib\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community faiss-cpu rank_bm25 hashlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "openai_api= userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "SO35O9ZD0Gwk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Dict\n",
        "import hashlib"
      ],
      "metadata": {
        "id": "QyvmXLbO0SoX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize components\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0,openai_api_key=openai_api)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=openai_api)"
      ],
      "metadata": {
        "id": "WFhBqEt60bjy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate documents with unique IDs\n",
        "raw_documents = [\n",
        "    \"The French Revolution began in 1789 with the storming of the Bastille.\",\n",
        "    \"Louis XVI was executed in 1793 during the Reign of Terror.\",\n",
        "    \"The Revolution led to the rise of Napoleon Bonaparte in 1799.\"\n",
        "]\n",
        "\n",
        "# Create documents with hashed IDs\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=content,\n",
        "        metadata={\"id\": hashlib.md5(content.encode()).hexdigest()}\n",
        "    ) for content in raw_documents\n",
        "]\n",
        "\n",
        "# Preprocess documents with chunk-aware metadata\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "split_docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "hdQkpQF40owU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add chunk IDs to split documents\n",
        "for idx, doc in enumerate(split_docs):\n",
        "    doc.metadata[\"chunk_id\"] = f\"{doc.metadata['id']}_{idx}\"\n",
        "\n",
        "# 2. Fixed HybridRetriever implementation\n",
        "class HybridRetriever:\n",
        "    def __init__(self, sparse_retriever, dense_retriever):\n",
        "        self.sparse_retriever = sparse_retriever\n",
        "        self.dense_retriever = dense_retriever\n",
        "\n",
        "    def invoke(self, query: str) -> List[Document]:\n",
        "        sparse_docs = self.sparse_retriever.invoke(query)\n",
        "        dense_docs = self.dense_retriever.invoke(query)\n",
        "        return self._merge_results(sparse_docs, dense_docs)\n",
        "\n",
        "    def _merge_results(self, sparse: List[Document], dense: List[Document]) -> List[Document]:\n",
        "        all_docs = sparse + dense\n",
        "        seen = set()\n",
        "        return [\n",
        "            doc for doc in all_docs\n",
        "            if not (doc.metadata[\"chunk_id\"] in seen or seen.add(doc.metadata[\"chunk_id\"]))\n",
        "        ]\n",
        "\n",
        "# Create retrievers with chunk IDs\n",
        "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
        "bm25_retriever.k = 3\n",
        "\n",
        "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
        "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Initialize hybrid retriever\n",
        "hybrid_retriever = HybridRetriever(bm25_retriever, faiss_retriever)"
      ],
      "metadata": {
        "id": "-Hx07MtQ0tSW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. HyDE implementation\n",
        "hyde_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Generate a hypothetical answer to the following query, even if you don't know the answer.\n",
        "    Include relevant entities, dates, and concepts that would be found in authoritative documents.\n",
        "\n",
        "    Query: {query}\n",
        "    Hypothetical Answer:\"\"\"\n",
        ")\n",
        "\n",
        "# Define the HyDE workflow\n",
        "hyde_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        original_query=lambda x: x[\"query\"]\n",
        "    )\n",
        "    | {\n",
        "        \"query\": hyde_prompt | llm | StrOutputParser(),\n",
        "        \"original_query\": RunnablePassthrough()\n",
        "    }\n",
        "    | RunnableLambda(lambda x: {\n",
        "        \"hyde_query\": x[\"query\"],\n",
        "        \"original_query\": x[\"original_query\"]\n",
        "    })\n",
        ")\n",
        "\n",
        "# 4. Full HyDE RAG pipeline\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Answer the user's question using both the original query and retrieved documents.\n",
        "    Original Question: {original_query}\n",
        "    Retrieved Context: {context}\"\"\"),\n",
        "    (\"human\", \"Answer this question: {original_query}\")\n",
        "])\n",
        "\n",
        "full_hyde_chain = (\n",
        "    hyde_chain\n",
        "    | {\n",
        "        \"context\": lambda x: hybrid_retriever.invoke(x[\"hyde_query\"]),\n",
        "        \"original_query\": lambda x: x[\"original_query\"]\n",
        "    }\n",
        "    | {\n",
        "        \"context\": lambda x: \"\\n\\n\".join([doc.page_content for doc in x[\"context\"]]),\n",
        "        \"original_query\": lambda x: x[\"original_query\"]\n",
        "    }\n",
        "    | final_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "BssttxpF1B9Z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Test the implementation\n",
        "response = full_hyde_chain.invoke({\"query\": \"What were the main causes and consequences of the French Revolution?\"})\n",
        "print(\"HyDE Response:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07weHVsb1INO",
        "outputId": "ae095783-a664-4088-9172-35dfeaadb334"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HyDE Response:\n",
            " The main causes of the French Revolution included social inequality, economic hardship, and political discontent. The revolution began in 1789 with the storming of the Bastille and led to significant consequences such as the execution of Louis XVI in 1793 during the Reign of Terror. Additionally, the Revolution ultimately resulted in the rise of Napoleon Bonaparte in 1799, who played a significant role in shaping the future of France and Europe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Features:**\n",
        "**Hybrid Retrieval:**\n",
        "\n",
        "* Combines BM25 (sparse) and FAISS (dense) retrieval methods\n",
        "\n",
        "* Merges results while removing duplicates using document IDs\n",
        "\n",
        "\n",
        "**Hypothetical Document Embedding (HyDE):**\n",
        "\n",
        "* Generates hypothetical answers using GPT-3.5-turbo\n",
        "\n",
        "* Uses these hypothetical answers as queries for retrieval\n",
        "\n",
        "* Maintains original query context for final answer generation"
      ],
      "metadata": {
        "id": "3P_SM7GV2kei"
      }
    }
  ]
}