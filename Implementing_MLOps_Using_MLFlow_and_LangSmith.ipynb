{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction to MLOps**\n",
        "MLOps is a set of best practices for automating and managing the ML lifecycle, including:\n",
        "\n",
        "* Model versioning and tracking\n",
        "* Experimentation and reproducibility\n",
        "* Continuous integration/continuous deployment (CI/CD)\n",
        "* Model monitoring and performance tracking\n",
        "\n",
        "MLFlow and LangSmith are widely used for these tasks."
      ],
      "metadata": {
        "id": "tkV7su3LvMtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Overview of MLFlow**\n",
        "MLFlow is an open-source MLOps platform with four key components:\n",
        "\n",
        "**a. MLFlow Tracking**\n",
        "\n",
        "* Logs and tracks ML experiments (parameters, metrics, artifacts)\n",
        "* Supports multiple frameworks (TensorFlow, PyTorch, Scikit-learn)\n",
        "* Enables experiment comparison and visualization\n",
        "\n",
        "**b. MLFlow Projects**\n",
        "\n",
        "* Standardizes ML code using a project format (e.g., conda.yaml or requirements.txt)\n",
        "* Ensures reproducibility across different environments\n",
        "\n",
        "**c. MLFlow Models**\n",
        "\n",
        "* Packages models for deployment using a universal format\n",
        "* Supports multiple deployment targets (Docker, Kubernetes, cloud services)\n",
        "\n",
        "**d. MLFlow Registry**\n",
        "* Manages and version-controls models\n",
        "* Provides model lifecycle stages: staging, production, archived"
      ],
      "metadata": {
        "id": "Tl_OI13kvXuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Introduction to LangSmith**\n",
        "LangSmith is a debugging, monitoring, and evaluation platform specifically designed for LLM (Large Language Model) applications. It helps in:\n",
        "\n",
        "  * Tracing, debugging, and visualizing LLM execution paths\n",
        "  * Evaluating performance using metrics and logs\n",
        "  * Monitoring and improving GenAI models\n",
        "\n",
        "LangSmith is useful for GenAI applications that involve:\n",
        "\n",
        "  * LLM-based chatbots\n",
        "  * AI-driven content generation\n",
        "  * Text summarization and retrieval-augmented generation (RAG)"
      ],
      "metadata": {
        "id": "y_LWqwHEvxL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Implementing MLOps for GenAI with MLFlow and LangSmith**"
      ],
      "metadata": {
        "id": "qGNXHDRUwRHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lxOXvWjnuxXZ"
      },
      "outputs": [],
      "source": [
        "!pip install mlflow -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "_CYWn-8kwVi2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLflow tracking URI with SQLite\n",
        "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
        "\n",
        "# Start the MLflow server using subprocess\n",
        "subprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI, \"--port\", \"5000\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIX5C8-TwsHh",
        "outputId": "54d546da-19b0-40f2-ce46-d2593f6871ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['mlflow', 'ui', '--backend-store-uri', 'sqli...>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set MLflow tracking URI\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)"
      ],
      "metadata": {
        "id": "bRYB0B7cxHco"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set or create an experiment\n",
        "mlflow.set_experiment(\"GenAI Experiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw_ZANFuxKsO",
        "outputId": "91045db1-8010-4147-8919-f02b74648afa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/01/30 10:33:05 INFO mlflow.tracking.fluent: Experiment with name 'GenAI Experiment' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='/content/mlruns/1', creation_time=1738233185029, experiment_id='1', last_update_time=1738233185029, lifecycle_stage='active', name='GenAI Experiment', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langsmith\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3UmiiMxxXxw",
        "outputId": "4839260e-eb6f-40ee-a0dd-c3f7be91880d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.15)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai_api= userdata.get(\"OPENAI_API_KEY\")\n",
        "langsmith_api= userdata.get(\"langsmith_api\")"
      ],
      "metadata": {
        "id": "bID36E3ExZvI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# LangSmith variables\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = langsmith_api\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"genai_exp\"\n",
        "\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api\n"
      ],
      "metadata": {
        "id": "uHO4LvdNxsmF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langsmith.wrappers import wrap_openai\n",
        "from langsmith import traceable\n",
        "\n",
        "# Wrap OpenAI client with LangSmith tracking\n",
        "client = wrap_openai(openai.Client())"
      ],
      "metadata": {
        "id": "TmmBG3N1x48s"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@traceable  # Automatically trace this function\n",
        "def pipeline(user_input: str):\n",
        "    result = client.chat.completions.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
        "        model=\"gpt-3.5-turbo\"  # Specify the model\n",
        "    )\n",
        "    return result.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "IluzrDjpy7Bs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = pipeline(\"Hello, world!\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duxYV1S7y_vL",
        "outputId": "179de3de-9a8a-4f79-88d4-26afb144d563"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3oL6kpkazCFe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}