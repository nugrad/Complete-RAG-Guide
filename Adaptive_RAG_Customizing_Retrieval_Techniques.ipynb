{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "Adaptive Retrieval-Augmented Generation (Adaptive-RAG) is an advanced extension of RAG that dynamically customizes retrieval strategies based on the query type, domain, and context sensitivity. Unlike traditional RAG models that use fixed retrieval techniques, Adaptive-RAG optimizes retrieval by switching between different retrieval strategies or adjusting retrieval parameters in real-time.\n",
        "\n",
        "This approach improves retrieval efficiency, enhances relevance, and optimizes computational costs, making it ideal for multi-modal, multi-domain, and context-sensitive applications."
      ],
      "metadata": {
        "id": "tx0uV8te4F7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Key Concepts of Adaptive-RAG**\n",
        "Adaptive-RAG is built on three foundational principles:\n",
        "\n",
        "\n",
        "1. **Adaptive Retrieval Strategy Selection**\n",
        "  * Instead of using a single fixed retrieval approach (e.g., FAISS, BM25, or hybrid retrieval), Adaptive-RAG:\n",
        "\n",
        "  * Dynamically chooses between sparse (BM25, TF-IDF), dense (FAISS, OpenAI Embeddings), or hybrid retrieval.\n",
        "  * Uses query classification to identify optimal retrieval methods (e.g., factual, exploratory, opinion-based).\n",
        "  * Adjusts retrieval depth (k) based on the query's complexity.\n",
        "\n",
        "2. **Context-Aware Query Expansion**\n",
        "\n",
        "  * Uses query rewriting techniques with LLMs to expand, refine, or simplify the query.\n",
        "  * Helps in cases where user queries are ambiguous or too broad.\n",
        "  * Uses self-querying retrievers to reformat the query before retrieval.\n",
        "\n",
        "3. **Dynamic Retrieval Weighting**\n",
        "\n",
        "  * Assigns dynamic weights to retrieval results based on:\n",
        "    * Confidence scores (how well documents match the query).\n",
        "    * Semantic similarity (using cosine similarity on embeddings).\n",
        "    * Keyword overlap (BM25 score-based filtering).\n",
        "    \n",
        "By balancing retrieval results, Adaptive-RAG ensures more diverse and relevant responses."
      ],
      "metadata": {
        "id": "UkLXoaEC4LKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Core**\n",
        "**Adaptive-RAG dynamically selects the best retrieval strategy for each query:**\n",
        "\n",
        "1. **Query Classification:** Determines if a query needs retrieval (and which type)\n",
        "\n",
        "2. **Retrieval Options:**\n",
        "  * **Dense Retrieval:** For semantic/similarity searches (e.g., \"Explain climate change\")\n",
        "\n",
        "  * **Sparse Retrieval:** For keyword-based searches (e.g., \"GDP of France in 2023\")\n",
        "\n",
        "  * **Hybrid:** Combines both methods for complex queries\n",
        "\n",
        "3. **Fallback Mechanism:** Uses the LLM's internal knowledge if no relevant documents are found"
      ],
      "metadata": {
        "id": "TYBqUzFl5NZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation**"
      ],
      "metadata": {
        "id": "xqP2KVsl51xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-openai langchain-community faiss-cpu rank_bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xdtgXtI46v3",
        "outputId": "5dec920f-9fef-47cb-d135-4433fe816b41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "openai_api= userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "t0U7CgoD5_gr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "a6H14Zr36JaT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize components\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0,openai_api_key=openai_api)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",openai_api_key=openai_api)"
      ],
      "metadata": {
        "id": "M926OA746PxV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents\n",
        "docs = [\n",
        "    \"The French Revolution began in 1789.\",\n",
        "    \"Napoleon became Emperor in 1804.\",\n",
        "    \"World War II ended in 1945.\"\n",
        "]"
      ],
      "metadata": {
        "id": "tRSzEUb46SYW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create retrievers\n",
        "# Sparse (keyword-based)\n",
        "bm25_retriever = BM25Retriever.from_texts(docs)\n",
        "bm25_retriever.k = 2\n",
        "\n",
        "# Dense (semantic)\n",
        "vectorstore = FAISS.from_texts(docs, embeddings)\n",
        "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "MJR2q1Fx6YCF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Query classifier\n",
        "def classify_query(query: str) -> str:\n",
        "    prompt = \"\"\"Classify the query type:\n",
        "    - 'simple' (needs no retrieval, e.g., \"Hello!\")\n",
        "    - 'fact' (needs keyword search, e.g., \"When did WWII end?\")\n",
        "    - 'complex' (needs semantic search, e.g., \"Explain the French Revolution\")\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt.format(query=query)).content.lower()\n",
        "    return \"complex\" if \"complex\" in response else \"fact\" if \"fact\" in response else \"simple\""
      ],
      "metadata": {
        "id": "gRgHsyOu6gGq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Adaptive retriever\n",
        "def adaptive_retrieve(query: str):\n",
        "    query_type = classify_query(query)\n",
        "\n",
        "    if query_type == \"simple\":\n",
        "        return []  # No retrieval needed\n",
        "    elif query_type == \"fact\":\n",
        "        return bm25_retriever.invoke(query)\n",
        "    else:\n",
        "        return faiss_retriever.invoke(query)"
      ],
      "metadata": {
        "id": "XACZQP1k6mg7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. QA chain\n",
        "template = \"\"\"Answer using ONLY these documents:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=lambda x: adaptive_retrieve(x[\"query\"])\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "8LSFTwql6piq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Test\n",
        "queries = [\n",
        "    \"Hello! How are you?\",          # Simple (no retrieval)\n",
        "    \"When did WWII end?\",           # Fact (BM25)\n",
        "    \"Explain the French Revolution\" # Complex (FAISS)\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"Query: {query}\")\n",
        "    print(\"Answer:\", chain.invoke({\"query\": query}))\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPEFVGDO6sTe",
        "outputId": "f82640da-1971-4ba1-acfe-0809be97b4e6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Hello! How are you?\n",
            "Answer: I am doing well, thank you for asking.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: When did WWII end?\n",
            "Answer:  WWII ended in 1945.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Query: Explain the French Revolution\n",
            "Answer: The French Revolution began in 1789. Napoleon became Emperor in 1804.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Components**\n",
        "1. **Query Classifier:**\n",
        "\n",
        "\n",
        "```\n",
        "def classify_query(query: str) -> str:\n",
        "    # Uses LLM to categorize queries\n",
        "```\n",
        "2. **Retrieval Router:**\n",
        "\n",
        "\n",
        "```\n",
        "def adaptive_retrieve(query: str):\n",
        "    # Selects retrieval method based on query type\n",
        "```\n",
        "3. **Fallback Mechanism:**\n",
        "\n",
        "\n",
        "```\n",
        "template = \"\"\"Answer using ONLY these documents:\n",
        "{context}  # Empty list for simple queries\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vi27maRi6-oU"
      }
    }
  ]
}