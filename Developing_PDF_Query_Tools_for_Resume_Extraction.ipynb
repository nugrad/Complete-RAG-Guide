{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **üöÄ Introduction**\n",
        "With the increasing number of job applications, HR teams and recruiters need automated resume processing tools to extract relevant information from resumes efficiently. PDF Query Tools powered by Generative AI (GenAI), NLP, and Vector Databases allow structured extraction and querying of resume data."
      ],
      "metadata": {
        "id": "Lgm2BL5RlfPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üìå 1Ô∏è‚É£ Understanding Resume Extraction**\n",
        "**üîπ What is Resume Extraction?**\n",
        "\n",
        "Resume extraction is the process of automatically identifying, parsing, and structuring key details from resumes. This includes:\n",
        "\n",
        "‚úÖ Personal Information (Name, Email, Phone)\n",
        "\n",
        "‚úÖ Education (Degrees, Universities, Graduation Years)\n",
        "\n",
        "‚úÖ Work Experience (Companies, Job Titles, Responsibilities)\n",
        "\n",
        "‚úÖ Skills (Programming, Soft Skills, Certifications)\n",
        "\n",
        "‚úÖ Projects & Achievements\n",
        "\n",
        "**üîπ Why Automate Resume Parsing?**\n",
        "\n",
        "‚úÖ Speeds up recruitment (Extracts relevant details in seconds)\n",
        "\n",
        "‚úÖ Standardizes resume formats (Handles different resume templates)\n",
        "\n",
        "‚úÖ Improves searchability (Finds candidates based on skills, experience, etc.)\n",
        "\n",
        "‚úÖ Integrates with ATS (Applicant Tracking Systems)"
      ],
      "metadata": {
        "id": "foPAx92cl4sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**üìå 2Ô∏è‚É£ Applications of Generative AI in Resume Processing**\n",
        "GenAI enhances resume extraction with:\n",
        "\n",
        "üìù Resume Summarization: Generates a summary of key points\n",
        "\n",
        "üîç Intelligent Search (Semantic Queries): Finds resumes based on skill/experience\n",
        "\n",
        "üìä Candidate Ranking: Scores resumes based on job fit\n",
        "\n",
        "üîÑ Resume Formatting & Structuring: Converts unstructured resumes into structured data"
      ],
      "metadata": {
        "id": "Fy-oQputmkQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Real-World Applications**\n",
        "\n",
        "üîπ **1. AI-Powered Recruitment**\n",
        "* Quickly find the best resumes for a job opening.\n",
        "* Automate screening and ranking.\n",
        "\n",
        "üîπ **2. HR Resume Management**\n",
        "\n",
        "* Organize candidate resumes in a searchable database.\n",
        "* Query based on specific skills or experience.\n",
        "\n",
        "üîπ **3. Freelance/Job Portals**\n",
        "\n",
        "* Help recruiters find freelancers based on project history & expertise.\n",
        "\n",
        "üîπ **4. Education & Scholarship Screening**\n",
        "\n",
        "* Extract information from academic resumes for scholarships."
      ],
      "metadata": {
        "id": "tcIqhHXSmzCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation**"
      ],
      "metadata": {
        "id": "HoaXQzSZndqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DMkOFLo3jg3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec58f644-3847-4579-ab91-d4486e8c017d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Using cached langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.33 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.59.9)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.16 (from langchain-community)\n",
            "  Downloading langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (0.3.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-0.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.17-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pydantic-settings, dataclasses-json, langchain-core, langchain-openai, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.31\n",
            "    Uninstalling langchain-core-0.3.31:\n",
            "      Successfully uninstalled langchain-core-0.3.31\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.15\n",
            "    Uninstalling langchain-0.3.15:\n",
            "      Successfully uninstalled langchain-0.3.15\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.17 langchain-community-0.3.16 langchain-core-0.3.33 langchain-openai-0.3.3 marshmallow-3.26.0 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "\n",
        "openai_api= userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "hQohhFM3osSz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict, List\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "import re\n",
        "import json\n",
        "\n",
        "class ResumeParser:\n",
        "    def __init__(self, openai_api_key: str):\n",
        "        \"\"\"Initialize the resume parser with OpenAI API key.\"\"\"\n",
        "        self.llm = ChatOpenAI(\n",
        "            temperature=0,\n",
        "            model_name=\"gpt-3.5-turbo\",\n",
        "            openai_api_key=openai_api\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except:\n",
        "            os.system(\"python -m spacy download en_core_web_sm\")\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        self.matching_template = PromptTemplate(\n",
        "            input_variables=[\"resume_text\", \"job_description\"],\n",
        "            template=\"\"\"\n",
        "            Analyze the following resume against the job description and provide a detailed matching analysis.\n",
        "\n",
        "            Resume:\n",
        "            {resume_text}\n",
        "\n",
        "            Job Description:\n",
        "            {job_description}\n",
        "\n",
        "            Provide your response in the following JSON format:\n",
        "            {{\n",
        "                \"overall_match_percentage\": <number between 0 and 100>,\n",
        "                \"matching_requirements\": [<list of specific matching skills and requirements>],\n",
        "                \"missing_requirements\": [<list of specific missing or partial skills and requirements>],\n",
        "                \"recommendation\": \"<detailed recommendation based on the analysis>\"\n",
        "            }}\n",
        "\n",
        "            Ensure all fields have valid values and the response is in proper JSON format.\n",
        "            Be specific about technical skills, experience, and qualifications in your analysis.\n",
        "            Consider both exact matches and related/transferable skills.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        self.matching_chain = LLMChain(llm=self.llm, prompt=self.matching_template)\n",
        "\n",
        "    def load_resume(self, file_path: str) -> str:\n",
        "        \"\"\"Load resume from file.\"\"\"\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        if file_extension == '.pdf':\n",
        "            loader = PyPDFLoader(file_path)\n",
        "            pages = loader.load()\n",
        "            text = ' '.join([page.page_content for page in pages])\n",
        "        elif file_extension in ['.txt', '.doc', '.docx']:\n",
        "            loader = TextLoader(file_path)\n",
        "            text = loader.load()[0].page_content\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Please provide PDF, TXT, DOC, or DOCX file.\")\n",
        "\n",
        "        return text\n",
        "\n",
        "    def match_resume_with_job(self, resume_text: str, job_description: str) -> Dict:\n",
        "        \"\"\"Match resume against job description with guaranteed response.\"\"\"\n",
        "        try:\n",
        "            # Get match analysis from LLM\n",
        "            result = self.matching_chain.run({\n",
        "                \"resume_text\": resume_text,\n",
        "                \"job_description\": job_description\n",
        "            })\n",
        "\n",
        "            # Parse JSON response\n",
        "            try:\n",
        "                match_result = json.loads(result)\n",
        "            except json.JSONDecodeError:\n",
        "                # If JSON parsing fails, try to clean the response\n",
        "                # Remove any markdown formatting or extra text\n",
        "                clean_result = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
        "                if clean_result:\n",
        "                    match_result = json.loads(clean_result.group(0))\n",
        "                else:\n",
        "                    raise ValueError(\"Could not parse LLM response as JSON\")\n",
        "\n",
        "            # Validate and ensure all required fields are present\n",
        "            required_fields = {\n",
        "                'overall_match_percentage': 0,\n",
        "                'matching_requirements': [],\n",
        "                'missing_requirements': [],\n",
        "                'recommendation': 'No specific recommendation provided.'\n",
        "            }\n",
        "\n",
        "            for field, default_value in required_fields.items():\n",
        "                if field not in match_result or match_result[field] is None:\n",
        "                    match_result[field] = default_value\n",
        "\n",
        "            # Ensure percentage is a number between 0 and 100\n",
        "            try:\n",
        "                match_result['overall_match_percentage'] = float(match_result['overall_match_percentage'])\n",
        "                match_result['overall_match_percentage'] = max(0, min(100, match_result['overall_match_percentage']))\n",
        "            except (ValueError, TypeError):\n",
        "                match_result['overall_match_percentage'] = 0\n",
        "\n",
        "            # Ensure lists are actually lists\n",
        "            if not isinstance(match_result['matching_requirements'], list):\n",
        "                match_result['matching_requirements'] = []\n",
        "            if not isinstance(match_result['missing_requirements'], list):\n",
        "                match_result['missing_requirements'] = []\n",
        "\n",
        "            # Ensure recommendation is a string\n",
        "            if not isinstance(match_result['recommendation'], str):\n",
        "                match_result['recommendation'] = str(match_result['recommendation'])\n",
        "\n",
        "            return match_result\n",
        "\n",
        "        except Exception as e:\n",
        "            # Return a valid response structure even in case of error\n",
        "            return {\n",
        "                'overall_match_percentage': 0,\n",
        "                'matching_requirements': [],\n",
        "                'missing_requirements': ['Could not analyze requirements due to error'],\n",
        "                'recommendation': f'Error during analysis: {str(e)}. Please try again.'\n",
        "            }\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    OPENAI_API_KEY = openai_api\n",
        "    parser = ResumeParser(OPENAI_API_KEY)\n",
        "\n",
        "    # Example job description\n",
        "    job_description = \"\"\"\n",
        "    We are seeking a Machine Learning Engineer with:\n",
        "    - Strong Python programming skills\n",
        "    - Experience with ML frameworks (TensorFlow, PyTorch)\n",
        "    - Knowledge of NLP and Computer Vision\n",
        "    - Experience with REST APIs and Flask/FastAPI\n",
        "    - Database experience (SQL, NoSQL)\n",
        "    - Version control with Git\n",
        "    \"\"\"\n",
        "\n",
        "    # Load resume text from file or use provided text\n",
        "    resume_text = \"/content/HamzaJafri-Resume.pdf\"\n",
        "\n",
        "    # Get match analysis\n",
        "    match_result = parser.match_resume_with_job(resume_text, job_description)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n=== Match Analysis ===\")\n",
        "    print(f\"Overall Match: {match_result['overall_match_percentage']}%\")\n",
        "\n",
        "    print(\"\\nMatching Requirements:\")\n",
        "    for req in match_result['matching_requirements']:\n",
        "        print(f\"- {req}\")\n",
        "\n",
        "    print(\"\\nMissing Requirements:\")\n",
        "    for req in match_result['missing_requirements']:\n",
        "        print(f\"- {req}\")\n",
        "\n",
        "    print(f\"\\nRecommendation: {match_result['recommendation']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYrlifSso-7q",
        "outputId": "a7f44a21-9052-4c87-8509-6e85d4366a36"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Match Analysis ===\n",
            "Overall Match: 70.0%\n",
            "\n",
            "Matching Requirements:\n",
            "- Strong Python programming skills\n",
            "- Experience with ML frameworks (TensorFlow, PyTorch)\n",
            "- Knowledge of NLP and Computer Vision\n",
            "- Database experience (SQL, NoSQL)\n",
            "\n",
            "Missing Requirements:\n",
            "- Experience with REST APIs and Flask/FastAPI\n",
            "- Version control with Git\n",
            "\n",
            "Recommendation: The candidate's resume shows a strong match with the key requirements for the Machine Learning Engineer position. They have demonstrated strong Python programming skills and experience with ML frameworks like TensorFlow and PyTorch. Their knowledge of NLP and Computer Vision is also a good fit for the role. However, they are missing experience with REST APIs and Flask/FastAPI, as well as version control with Git. It is recommended that the candidate gains experience in these areas to further enhance their qualifications for the position.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEWbHZ66pGdt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmEwsLY6p5rO",
        "outputId": "dc805fdd-8974-469d-857f-b6f516720ddb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/298.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.8/298.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61Wn31OFpr5n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3wMG7Prp1ef"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}